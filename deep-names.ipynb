{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Keras to Identify Authors\n",
    "\n",
    "Much of what I'm doing in this notebook comes from François Chollet, _Deep Learning with Python_, Second Edition (Manning, 2021).\n",
    "\n",
    "First, I need to load the data, which I have prepared ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>dll_author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acosta, José de, 1540-1600</td>\n",
       "      <td>A5598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seneca, Lucius Annaeus, approximately 55 B.C.-...</td>\n",
       "      <td>A4920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gregory, Saint, Bishop of Tours, 538-594</td>\n",
       "      <td>A5257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Keil, Henricus, 1822-1894</td>\n",
       "      <td>A3509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ruusbroec, Jan van, 1293-1381</td>\n",
       "      <td>A4218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              author dll_author_id\n",
       "0                        Acosta, José de, 1540-1600         A5598\n",
       "1  Seneca, Lucius Annaeus, approximately 55 B.C.-...         A4920\n",
       "2           Gregory, Saint, Bishop of Tours, 538-594         A5257\n",
       "3                          Keil, Henricus, 1822-1894         A3509\n",
       "4                      Ruusbroec, Jan van, 1293-1381         A4218"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('output/all_names_deduplicated.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest string: 254\n",
      "Average string length: 29.07211951010219\n",
      "Average number of words per author: 3.90802714720337\n",
      "The total number of authors: 25638\n",
      "Ratio of words per author to mean length: 6560.343373854722\n"
     ]
    }
   ],
   "source": [
    "# Investigate the data\n",
    "\n",
    "# The length of the longest string in the 'author' column\n",
    "print(f\"Longest string: {df['author'].str.len().max()}\")\n",
    "\n",
    "# Get the average length\n",
    "print(f\"Average string length: {df['author'].str.len().mean()}\")\n",
    "\n",
    "# Average number of words per author\n",
    "df2 = df.copy()\n",
    "df2['author_words'] = df2['author'].apply(lambda x: len(x.split()))\n",
    "print(f\"Average number of words per author: {df2['author_words'].mean()}\")\n",
    "\n",
    "# The number of author strings\n",
    "print(f\"The total number of authors: {len(df['author'])}\")\n",
    "\n",
    "# Ratio of of number of records and mean length\n",
    "ratio = len(df['author'])/df2['author_words'].mean()\n",
    "print(f\"Ratio of words per author to mean length: {ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The goal of the model is to apply the correct DLL ID to the name of an author.\n",
    "\n",
    "I'll need training and validation data for the author names and DLL ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Flatten, Dense\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['dll_author_id'])\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['author'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure X_train is a 1D array\n",
    "X_train = X_train.to_numpy().reshape(-1)\n",
    "X_val = X_val.to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text\n",
    "\n",
    "Since Keras can't process text, I will also need to convert the text into vectors.\n",
    "\n",
    "I'll use the `TextVectorization` layer in Keras to preprocess the author strings instead of manually normalizing and vectorizing them.\n",
    "\n",
    "`max_tokens` is set to 20000 because, according to Chollet (390), “In general, 20,000 is the right vocabulary size for text classification.” \n",
    "\n",
    "The `output_mode` is `int` because we want numbers, not text.\n",
    "\n",
    "The `output_sequence_length` is set at 254, the length of the longest string in the 'authors' column. I found that by doing `df['author'].str.len().max()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TextVectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=254\n",
    ")\n",
    "\n",
    "# Adapt the vectorization layer to the training data\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Making the model\n",
    "\n",
    "I'm using a sequential model in accordance with Chollet's \"golden constant\" for determining when to use a bag-of-words model or a sequence model:\n",
    "\n",
    ">[Y]ou should pay close attention to the ratio between the number of samples in your training data and the mean number of words per sample …. If that ratio is small—less than 1,500—then the bag-of-bigrams model will perform better (and as a bonus, it will be much faster to train and to iterate on too). If that ratio is higher than 1,500, then you should go with a sequence model. In other words, sequence models work best when lots of training data is available and when each sample is relatively short. (421)\n",
    "\n",
    "The number of records overall is 25638, and the average number of words per record is 3.90802714720337, which works out to a ratio of approximately 6560. That isn't just the training data, I know, but this seems like a reasonable path to follow.\n",
    "\n",
    "In the model itself, the parameters are:\n",
    "\n",
    "- `vectorize_layer`: This turns the text into data that Keras can process. It normalizes and tokenizes the text data, then it turns that data into vectors.\n",
    "- `Embedding(input_dim=17719, output_dim=128)`: According to Chollet (398), word embeddings “map human language into a structured geometric space.” In other words, they “pack more information into far fewer dimensions.” He goes on to explain (401) that “The `Embedding` layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, looks up these integers in an internal dictionary, and returns the associated vectors.” The `input_dim` is set to the size of the vocabulary, which I found by doing `len(vectorize_layer.get_vocabulary())`. The `output_dim` parameter is set to a standard default of 128.\n",
    "- `Flatten()`: This function reduces the number of dimensions of the data to 2, which is what the `Dense` layer expects.\n",
    "- `Dense(128, activation='relu')`: This is one of the dense layers of the process. It has 128 'neurons' for processing the data. 128 is a standard \"Goldilocks\" number of neurons—not too many, not too few. A good place to start. The `relu` in `activation` refers to a standard function known as Rectified Linear Units.\n",
    "- `Dense(len(label_encoder.classes_), activation='softmax')`: This adds another dense layer, with the number of neurons set to the number of DLL Identifiers. This accords with Chollet's advice (151): \"If you’re trying to classify data points among N classes, your model should end with a Dense layer of size N.\" Chollet (151) recommends the `softmax` function for single-label, multi-class problems like this one since \"it will output a probability distribution over the N output classes.\" In other words, the probabilities will add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(input_dim=17719, output_dim=128, mask_zero=True),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model\n",
    "\n",
    "The `compile()` method assembles the model according to whatever parameters it receives. \n",
    "\n",
    "The optimizer determines how efficiently the model learns. Keras has several optimizers available. I selected `adam` because it is a popular choice for this sort of problem, and because it combines a lot of the best features of two other popular optimizers, `AdaGrad` and `RMSProp`.\n",
    "\n",
    "The loss parameter measures \"how far [the] output is from what you expected” (Chollet 31). I've selected `sparse_categorical_crossentropy` because Chollet (151) demonstrates that it \"is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the model and the true distribution of the targets.”\n",
    "\n",
    "The metric I'm interested in is, of course, accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "641/641 [==============================] - 45s 71ms/step - loss: 0.3189 - accuracy: 0.9455 - val_loss: 2.7886 - val_accuracy: 0.6624\n",
      "Epoch 2/10\n",
      "641/641 [==============================] - 45s 70ms/step - loss: 0.1036 - accuracy: 0.9848 - val_loss: 2.9019 - val_accuracy: 0.6732\n",
      "Epoch 3/10\n",
      "641/641 [==============================] - 46s 72ms/step - loss: 0.0385 - accuracy: 0.9952 - val_loss: 2.8881 - val_accuracy: 0.6899\n",
      "Epoch 4/10\n",
      "641/641 [==============================] - 45s 71ms/step - loss: 0.0196 - accuracy: 0.9969 - val_loss: 2.9468 - val_accuracy: 0.6936\n",
      "Epoch 5/10\n",
      "641/641 [==============================] - 46s 71ms/step - loss: 0.0120 - accuracy: 0.9979 - val_loss: 2.8455 - val_accuracy: 0.7001\n",
      "Epoch 6/10\n",
      "641/641 [==============================] - 46s 71ms/step - loss: 0.0105 - accuracy: 0.9981 - val_loss: 3.0328 - val_accuracy: 0.6923\n",
      "Epoch 7/10\n",
      "641/641 [==============================] - 45s 71ms/step - loss: 0.0095 - accuracy: 0.9987 - val_loss: 3.0611 - val_accuracy: 0.6919\n",
      "Epoch 8/10\n",
      "641/641 [==============================] - 45s 71ms/step - loss: 0.0113 - accuracy: 0.9980 - val_loss: 3.0350 - val_accuracy: 0.6952\n",
      "Epoch 9/10\n",
      "641/641 [==============================] - 45s 70ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 2.9649 - val_accuracy: 0.6960\n",
      "Epoch 10/10\n",
      "641/641 [==============================] - 45s 71ms/step - loss: 0.0113 - accuracy: 0.9978 - val_loss: 3.2775 - val_accuracy: 0.6901\n"
     ]
    }
   ],
   "source": [
    "# Save the best performing model\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# callbacks = [ModelCheckpoint(\"authors\",save_best_only=True,save_format='tf')]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 1s 5ms/step - loss: 3.2775 - accuracy: 0.6901\n",
      "Validation Accuracy: 69.01%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the validation set\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 254)\n"
     ]
    }
   ],
   "source": [
    "# Example of predicting a new author name\n",
    "new_author = \"Keil, Henricus, 1822-1894\"\n",
    "\n",
    "# Ensure the input is in the correct shape (list of strings)\n",
    "# This must be a list with a single string, so it becomes a 1D tensor of shape (1,)\n",
    "vectorized_input = vectorize_layer(tf.constant([new_author]))\n",
    "\n",
    "print(vectorized_input.shape)  # This should output something like (1, 254)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 573, in _preprocess\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'text_vectorization' (type TextVectorization).\n    \n    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 254) with rank=2\n    \n    Call arguments received by layer 'text_vectorization' (type TextVectorization):\n      • inputs=tf.Tensor(shape=(None, 254), dtype=string)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Predict using the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_probabilities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(vectorized_input)\n\u001b[1;32m      3\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m predicted_probabilities\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get the index of the highest probability\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Map the predicted label back to the original dll_author_id\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/73/ldt1v6c15m52k8nzfbwmbvg00000gn/T/__autograph_generated_file3nmywviz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 573, in _preprocess\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'text_vectorization' (type TextVectorization).\n    \n    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 254) with rank=2\n    \n    Call arguments received by layer 'text_vectorization' (type TextVectorization):\n      • inputs=tf.Tensor(shape=(None, 254), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict using the model\n",
    "predicted_probabilities = model.predict(vectorized_input)\n",
    "predicted_label = predicted_probabilities.argmax(axis=-1)  # Get the index of the highest probability\n",
    "\n",
    "# Map the predicted label back to the original dll_author_id\n",
    "predicted_id = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "print(f\"Predicted dll_author_id: {predicted_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted dll_author_id: A3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjhuskey/anaconda3/envs/aiml/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Example of predicting a new author name\n",
    "new_author = \"Hall, Joseph, 1574-1656\"\n",
    "\n",
    "# Pass the raw string to the model\n",
    "predicted_probabilities = model.predict([new_author])  # Pass as a list with one item\n",
    "\n",
    "# Get the index of the highest probability\n",
    "predicted_label = predicted_probabilities.argmax(axis=-1)\n",
    "\n",
    "# Map the predicted label back to the original dll_author_id\n",
    "predicted_id = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "print(f\"Predicted dll_author_id: {predicted_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring HathiTrust Metadata with NLP and Machine Learning with SpaCY\n",
    "\n",
    "This is a Jupyter notebook for working with metadata downloaded from HathiTrust from a search with the following criteria:\n",
    "\n",
    "    - Title: liber libri libris libro OR All Fields: opus opera operibus OR Title: carmen carmina carminibus\n",
    "    - Language: (Latin)\n",
    "    - Original Format: (Book)\n",
    "\n",
    "## Working assumptions\n",
    "\n",
    "- Some records will not be candidates for inclusion in the DLL Catalog because they are editions of Greek works. Such editions traditionally have Latin titles and introductorty materials.\n",
    "- Some records will not have corresponding authority and/or work files in the DLL Catalog.\n",
    "- The names of many authors will be present in many variant spellings and forms.\n",
    "- The titles of works will be particularly difficult to parse, since they will be along the lines of _opera omnia_, vel sim.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Having used purely Python and Pandas methods in a previous attempt, in this notebook I'll see what can be accomplished using SpaCY, a Python library for Natural Language Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.training.iob_utils import offsets_to_biluo_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV file with variant names from the DLL Catalog's authority records\n",
    "df = pd.read_csv('input/variant-names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3229 entries, 0 to 3228\n",
      "Data columns (total 13 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   DLL Identifier                  3227 non-null   object \n",
      " 1   Authorized Name                 3227 non-null   object \n",
      " 2   Author Name English             1252 non-null   object \n",
      " 3   Author Name Latin               2197 non-null   object \n",
      " 4   Author Name Native Language     1045 non-null   object \n",
      " 5   BNE URL                         562 non-null    object \n",
      " 6   BNF URL                         1361 non-null   object \n",
      " 7   DNB URL                         2068 non-null   object \n",
      " 8   ICCU URL                        407 non-null    object \n",
      " 9   ISNI URL                        1983 non-null   object \n",
      " 10  Other Alternative Name Form(s)  38 non-null     object \n",
      " 11  Perseus Name                    827 non-null    object \n",
      " 12  Wikidata URL                    0 non-null      float64\n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 328.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Albert, of Aachen, active 11th-12th century', {'entities': [(0, 7, 'PERSON'), (8, 10, 'PERSON'), (11, 18, 'PERSON'), (19, 25, 'PERSON'), (26, 35, 'PERSON'), (36, 43, 'PERSON')]}), ('Albert of Aix-la-Chapelle/Albert of Aachen', {'entities': [(0, 6, 'PERSON'), (7, 9, 'PERSON'), (10, 32, 'PERSON'), (33, 35, 'PERSON'), (36, 42, 'PERSON')]}), ('Albertus Aquensis', {'entities': [(0, 8, 'PERSON'), (9, 17, 'PERSON')]}), (\"Albert d'Aix-la-Chapelle\", {'entities': [(0, 6, 'PERSON'), (7, 24, 'PERSON')]}), ('Marullo Tarcaniota, Michele', {'entities': [(0, 7, 'PERSON'), (8, 19, 'PERSON'), (20, 27, 'PERSON')]})]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract training data from the DataFrame\n",
    "def create_training_data(df):\n",
    "    training_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        standard_name = row['Authorized Name']\n",
    "        alternative_names = row[2:6].dropna().tolist()  # Drop NaN values and convert to list\n",
    "        all_names = [standard_name] + alternative_names\n",
    "        for name in all_names:\n",
    "            if not isinstance(name, str):  # Ensure the value is a string\n",
    "                continue\n",
    "            start_idx = 0\n",
    "            entities = []\n",
    "            for part in name.split():\n",
    "                end_idx = start_idx + len(part)\n",
    "                entities.append((start_idx, end_idx, \"PERSON\"))\n",
    "                start_idx = end_idx + 1  # +1 for the space\n",
    "            training_data.append((name, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "training_data = create_training_data(df)\n",
    "print(training_data[:5])  # Print the first 5 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjhuskey/anaconda3/envs/myspacy/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Antonio Martínez  de Cala\" with entities \"[(0, 7, 'PERSON'), (8, 16, 'PERSON'), (17, 19, 'PE...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize a blank NER model\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add the PERSON label to the NER model\n",
    "ner.add_label(\"PERSON\")\n",
    "\n",
    "# Verify and correct entity alignment\n",
    "def verify_and_correct_alignment(nlp, training_data):\n",
    "    corrected_training_data = []\n",
    "    for text, annotations in training_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        try:\n",
    "            tags = offsets_to_biluo_tags(doc, annotations['entities'])\n",
    "            corrected_entities = [(start, end, label) for (start, end, label), tag in zip(annotations['entities'], tags) if tag != '-']\n",
    "            corrected_training_data.append((text, {\"entities\": corrected_entities}))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in alignment: {e}\")\n",
    "            continue\n",
    "    return corrected_training_data\n",
    "\n",
    "training_data = verify_and_correct_alignment(nlp, training_data)\n",
    "\n",
    "# Convert the training data to spaCy's Example format\n",
    "examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Losses {'ner': 27583.331735253334}\n",
      "Iteration 1: Losses {'ner': 26159.876992583275}\n",
      "Iteration 2: Losses {'ner': 25988.22890651226}\n",
      "Iteration 3: Losses {'ner': 25131.86690801382}\n",
      "Iteration 4: Losses {'ner': 24238.963100135326}\n",
      "Iteration 5: Losses {'ner': 23550.002394080162}\n",
      "Iteration 6: Losses {'ner': 22483.592281639576}\n",
      "Iteration 7: Losses {'ner': 21491.744533151388}\n",
      "Iteration 8: Losses {'ner': 21078.74868634343}\n",
      "Iteration 9: Losses {'ner': 20057.840320557356}\n",
      "Iteration 10: Losses {'ner': 19570.887015789747}\n",
      "Iteration 11: Losses {'ner': 19197.53355795145}\n",
      "Iteration 12: Losses {'ner': 17831.028558790684}\n",
      "Iteration 13: Losses {'ner': 17661.25188705325}\n",
      "Iteration 14: Losses {'ner': 16407.887051343918}\n",
      "Iteration 15: Losses {'ner': 15755.708585716784}\n",
      "Iteration 16: Losses {'ner': 15273.423639848828}\n",
      "Iteration 17: Losses {'ner': 14983.133277595043}\n",
      "Iteration 18: Losses {'ner': 15690.668749511242}\n",
      "Iteration 19: Losses {'ner': 15240.339127972722}\n",
      "Virgil PERSON\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Train the NER model\n",
    "for i in range(20):  # Number of training iterations\n",
    "    losses = {}\n",
    "    nlp.update(examples, drop=0.5, sgd=optimizer, losses=losses)\n",
    "    print(f\"Iteration {i}: Losses {losses}\")\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(\"custom_ner_model\")\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(\"custom_ner_model\")\n",
    "\n",
    "# Test the model with a sample text\n",
    "test_text = \"Virgil\"\n",
    "doc = nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergil. PERSON\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample text\n",
    "test_text = \"Vergil.\"\n",
    "doc = nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['Authorized Name','DLL Identifier']]\n",
    "lookup_dict = df2.set_index('Authorized Name')['DLL Identifier'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HathiTrust metadata\n",
    "df3 = pd.read_csv('input/1908698974-1722799169.txt', sep='\\t')\n",
    "# Make a new dataframe with the required columns\n",
    "hathidata = df3[['author','title','imprint','pub_place','rights_date_used','handle_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the original 6017 authors, 5988 remain unreconciled.\n"
     ]
    }
   ],
   "source": [
    "# Use the lookup_dict created from the DataFrame with \"DLL Identifier\" and \"Authorized Name\"\n",
    "\n",
    "# Function to identify named entities and match to DLL identifiers\n",
    "def identify_and_match_author(nlp, text, lookup_dict):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            matched_name = ent.text\n",
    "            return lookup_dict.get(matched_name)\n",
    "    return None\n",
    "\n",
    "# Apply the function to the 'author' column and create a new column for DLL identifiers\n",
    "hathidata.loc[:,'dll_identifier'] = hathidata['author'].apply(lambda x: identify_and_match_author(nlp, x, lookup_dict))\n",
    "\n",
    "# Make the new dataframe\n",
    "unreconciled = hathidata[hathidata['dll_identifier'].isna()]\n",
    "# Use nunique() to count the unique values in the \"author\" column\n",
    "print(f\"Out of the original {hathidata['author'].nunique()} authors, {unreconciled['author'].nunique()} remain unreconciled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Horace', 'Commodianus.', 'Livy', 'Persius', 'Phaedrus',\n",
       "       'Tibullus', 'Virgil', 'Homer', 'Horace.', 'Martial', 'Apuleius.',\n",
       "       'Commodianus', 'Origen', 'Aristotle', 'Quintilian', 'Martial.',\n",
       "       'Persius.', 'Eugippius.', 'Apponius.', 'Apicius.', 'Juvenal',\n",
       "       'Apuleius', 'Lygdamus.', 'Apponius', 'Terence', 'Gaius.',\n",
       "       'Atilius Fortunatianus, active 4th century.', 'Defensor de Ligugé',\n",
       "       'Atilius Fortunatianus, 4th cent.'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identified = hathidata[~hathidata['dll_identifier'].isna()]\n",
    "identified['author'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUVENAL and Livy were identified. Weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
